{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOjrzRgWDvyl"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hqb6PIfeyi9",
        "outputId": "b21238b2-28cb-4a71-8457-fb88f3b19de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import ssl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# check if CUDA is available\n",
        "is_gpu_available = torch.cuda.is_available()\n",
        "\n",
        "if not is_gpu_available:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DIR_PATH = '/content/drive/MyDrive/Deep learning 05107255/ex2_316168061_313471526'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOIRwCg62eyL",
        "outputId": "1e7103e9-5428-429f-a3f3-6d372187a4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OR2tlv5PxtYT"
      },
      "outputs": [],
      "source": [
        "params = {'batch_size'    : 20,\n",
        "          'seq_length'    : 20,\n",
        "          'hidden_size'   : 200,\n",
        "          'num_layers'    : 2,\n",
        "          'embed_size'    : 200,\n",
        "          'dropout'       : 0.2,\n",
        "          'lr'            : 0.01,\n",
        "          'lr_decay'      : 1.3,\n",
        "          'max_grad_norm' : 2,\n",
        "          'epochs'        : 5\n",
        "          }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF4ND1uxD5YT"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = []\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self.word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word2idx)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "  def __init__(self, data_dir_path):\n",
        "    self.dictionary = Dictionary()\n",
        "    self.train = self.tokenize(os.path.join(data_dir_path, 'ptb.train.txt'))\n",
        "    self.valid = self.tokenize(os.path.join(data_dir_path, 'ptb.valid.txt'))\n",
        "    self.test = self.tokenize(os.path.join(data_dir_path, 'ptb.test.txt'))\n",
        "\n",
        "  def tokenize(self, fname):\n",
        "    with open(fname, 'r') as f:\n",
        "      data_str = f.read()\n",
        "      words = data_str.replace('\\n', '<eof>')\n",
        "      words = words.split()\n",
        "      ids = torch.LongTensor(len(words))\n",
        "      for i, word in enumerate(words):\n",
        "        self.dictionary.add_word(word)\n",
        "        ids[i] = self.dictionary.word2idx[word]\n",
        "    return ids\n",
        "\n",
        "corpus = Corpus(DIR_PATH)\n",
        "vocab_size = len(corpus.dictionary)"
      ],
      "metadata": {
        "id": "TiNbIadfQq1B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_generator(data, batch_size, seq_length):\n",
        "  mini_batch_size = batch_size * seq_length\n",
        "  n_mini_batches = len(data) // mini_batch_size\n",
        "  # Trucate data\n",
        "  # TODO add possibility to pad\n",
        "  data = data[:n_mini_batches * mini_batch_size]\n",
        "  data = data.reshape((batch_size, -1))\n",
        "  for i in range(0, data.size(1), seq_length):\n",
        "    x = data[:, i:i + seq_length]\n",
        "    y = torch.zeros_like(x)\n",
        "    if i + seq_length < data.size(1):\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], data[:, i + seq_length]\n",
        "    else:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], data[:, 0].roll(-1, 0)\n",
        "    yield x, y\n"
      ],
      "metadata": {
        "id": "xLTzJzvr3v5g"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07nDtn7U2qaD",
        "outputId": "774d922e-bc01-44af-ae91-60d38238aa75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/drive/MyDrive/Deep learning 05107255/ex2_316168061_313471526/ptb.train.txt, size of data = 929589\n",
            "46479\n",
            "torch.Size([929580])\n",
            "Loading /content/drive/MyDrive/Deep learning 05107255/ex2_316168061_313471526/ptb.valid.txt, size of data = 73760\n",
            "3688\n",
            "torch.Size([73760])\n",
            "Loading /content/drive/MyDrive/Deep learning 05107255/ex2_316168061_313471526/ptb.test.txt, size of data = 82430\n",
            "4121\n",
            "torch.Size([82420])\n"
          ]
        }
      ],
      "source": [
        "# train_file_name = f'{DIR_PATH}/ptb.train.txt'\n",
        "# val_file_name = f'{DIR_PATH}/ptb.valid.txt'\n",
        "# test_file_name = f'{DIR_PATH}/ptb.test.txt'\n",
        "\n",
        "# word2idx = {}\n",
        "\n",
        "# def load_data(fname, batch_size, dictionary={}):\n",
        "#   with open(fname, 'r') as f:\n",
        "#     data_str = f.read()\n",
        "#   data = data_str.replace('\\n', '<eof>')\n",
        "#   data = data.split()\n",
        "#   print(\"Loading {}, size of data = {}\".format(fname, len(data)))\n",
        "\n",
        "#   x = torch.LongTensor(len(data))\n",
        "#   vocab_idx = len(dictionary)\n",
        "#   for i in range(len(data)):\n",
        "#     if data[i] not in dictionary:\n",
        "#       dictionary[data[i]] = vocab_idx\n",
        "#       vocab_idx += 1\n",
        "#     x[i] = dictionary[data[i]]\n",
        "\n",
        "#   num_batches = x.size(0) // batch_size\n",
        "#   x = x[:num_batches * batch_size]\n",
        "#   return x.view(batch_size, -1), dictionary\n",
        "\n",
        "# train_data, word2idx = load_data(train_file_name, params['batch_size'])\n",
        "# val_data, word2idx = load_data(val_file_name, params['batch_size'], dictionary=word2idx)\n",
        "# test_data, word2idx = load_data(test_file_name, params['batch_size'], dictionary=word2idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGAzTMPz6l-w"
      },
      "source": [
        "# Define The Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "hN79J5ijq8As"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules import dropout\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout_prob, use_lstm=True):\n",
        "    super(Net, self).__init__()\n",
        "    self.n_hidden = hidden_size\n",
        "    self.n_layers = num_layers\n",
        "    self.use_lstm = use_lstm\n",
        "\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    if use_lstm:\n",
        "      self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if self.use_lstm:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    else:\n",
        "      hidden = weight.new(self.n_layers, batch_size, self.n_hidden).zero_()\n",
        "    return hidden\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    x = self.dropout(self.embed(x))\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    out = self.dropout(out)\n",
        "    out = out.reshape(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out, hidden\n",
        "\n",
        "def repackage_hidden(hidden):\n",
        "  if type(hidden) == torch.Tensor:\n",
        "    hidden = hidden.data\n",
        "  else:\n",
        "    hidden = tuple([h.data for h in hidden])\n",
        "  return hidden\n",
        "\n",
        "model = Net(vocab_size, params['embed_size'], params['hidden_size'], params['num_layers'], params['dropout']).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma = params['lr_decay'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxc25lbwECa8"
      },
      "source": [
        "# Functions For Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "gX3DdqCgCvy7"
      },
      "outputs": [],
      "source": [
        "def evaluate(net, data, criterion, params, is_gpu_available):\n",
        "  running_loss = 0.0\n",
        "  batch_size = params['batch_size']\n",
        "  hidden = net.init_hidden(batch_size)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for x, y in batch_generator(data, params['batch_size'], params['seq_length']):\n",
        "      if is_gpu_available:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      hidden = repackage_hidden(hidden)\n",
        "\n",
        "      output, hidden = net(x, hidden)\n",
        "      loss = criterion(output, y.reshape(batch_size*params['seq_length']))\n",
        "\n",
        "      running_loss += loss.item()\n",
        "  \n",
        "  valid_loss = running_loss / (len(data) // (params['seq_length'] * params['batch_size']))\n",
        "  return valid_loss\n",
        "\n",
        "def train(net, data, criterion, opt, params, is_gpu_available):\n",
        "  running_loss = 0.0\n",
        "  batch_size = params['batch_size']\n",
        "  hidden = net.init_hidden(batch_size)\n",
        "  net.train()\n",
        "  batch_gen = batch_generator(data, params['batch_size'], params['seq_length'])\n",
        "  for x, y in batch_gen:\n",
        "    if is_gpu_available:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    net.zero_grad()\n",
        "    output, hidden = net(x, hidden)\n",
        "    loss = criterion(output, y.reshape(batch_size*params['seq_length']))\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(net.parameters(), params['max_grad_norm'])\n",
        "    opt.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  \n",
        "  epoch_loss = running_loss / (len(data) // (params['seq_length'] * params['batch_size']))\n",
        "  return net, opt, epoch_loss\n",
        "\n",
        "def training_loop(train_data, val_data, net, opt, criterion, scheduler, params, is_gpu_available):\n",
        "  # set objects for storing metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_perp_vec = []\n",
        "    val_perp_vec = []\n",
        "    tb = SummaryWriter()\n",
        "\n",
        "    print(f'{datetime.now().time().replace(microsecond=0)} START')\n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(0, params['epochs']):\n",
        "      net, optimizer, train_loss = train(net, train_data, criterion, opt, params, is_gpu_available)\n",
        "      train_perp = np.exp(train_loss)\n",
        "      train_losses.append(train_loss)\n",
        "      train_perp_vec.append(train_perp)\n",
        "      scheduler.step()\n",
        "\n",
        "      # validation\n",
        "      val_loss = evaluate(net, val_data, criterion, params, is_gpu_available)\n",
        "      val_perp = np.exp(val_loss)\n",
        "      val_losses.append(val_loss)\n",
        "      val_perp_vec.append(val_perp)\n",
        "\n",
        "      tb.add_scalar(\"Train Loss\", train_loss, epoch)\n",
        "      tb.add_scalar(\"Valid Loss\", val_loss, epoch)\n",
        "      tb.add_scalar(\"Train Perplexity\", train_perp, epoch)\n",
        "      tb.add_scalar(\"Valid Perplexity\", val_perp, epoch)\n",
        "\n",
        "      print(\n",
        "          f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "          f'Epoch: {epoch}\\t'\n",
        "          f'Train loss: {train_loss:.4f}\\t'\n",
        "          f'Val loss: {val_loss:.4f}\\t'\n",
        "          f'Train perplexity: {train_perp:.4f}\\t'\n",
        "          f'Test perplexity: {val_perp:.4f}'\n",
        "        )\n",
        "    \n",
        "    tb.close()\n",
        "    return net, opt, (train_losses, val_losses), (train_perp_vec, val_perp_vec)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting & Helper Functions"
      ],
      "metadata": {
        "id": "dU_d2y4Rz4RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_and_valid_losses(train_losses, test_losses, train_type=None):\n",
        "  train_losses = np.array(train_losses) \n",
        "  test_losses = np.array(test_losses)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize = (8, 4.5))\n",
        "\n",
        "  ax.plot(train_losses, color='blue', label='Training loss') \n",
        "  ax.plot(test_losses, color='red', label='Validation loss')\n",
        "  title = \"Loss over epochs\"\n",
        "  if train_type:\n",
        "    title += \"\\n\" + train_type\n",
        "  ax.set(title=title, \n",
        "          xlabel='Epoch',\n",
        "          ylabel='Loss') \n",
        "  ax.legend()\n",
        "  plt.grid()\n",
        "  fig.show()\n",
        "    # --------------------------------------------------------------------------------------------------\n",
        "def plot_train_and_valid_perp(train_perp, valid_perp, train_type=None):\n",
        "  train_perp = np.array(train_perp) \n",
        "  valid_perp = np.array(valid_perp)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize = (8, 4.5))\n",
        "\n",
        "  ax.plot(train_perp, color='blue', label='Training Perplexity') \n",
        "  ax.plot(valid_perp, color='red', label='Validation Perplexity')\n",
        "  title = \"Perplexity over epochs\"\n",
        "  if train_type:\n",
        "    title += \"\\n\" + train_type\n",
        "  ax.set(title=title, \n",
        "          xlabel='Epoch',\n",
        "          ylabel='Perplexity') \n",
        "  ax.legend()\n",
        "  fig.show()\n",
        "  \n",
        "  return fig\n",
        "# --------------------------------------------------------------------------------------------------\n",
        "def save_model(model, optimizer, dir_path, train_type=None):\n",
        "  # create checkpints folder\n",
        "  checkpoint_folder = os.path.join(dir_path, 'checkpoints')\n",
        "  os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "  # save model\n",
        "  curr_datetime = datetime.now().strftime(\"%y_%m_%d_%H:%M:%S\")\n",
        "  model_name = f'{curr_datetime}'.replace(':', '_')\n",
        "  if train_type:\n",
        "    model_name = train_type + '_' + model_name\n",
        "    # model_name = train_type\n",
        "  ckpt_path = os.path.join(checkpoint_folder, f'{model_name}.ckpt')\n",
        "  torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'use_dropout': model.use_dropout,\n",
        "    'use_batchnorm': model.use_batchnorm,\n",
        "    \n",
        "  }, ckpt_path)\n",
        "\n",
        "  print(f'model saved to \"{ckpt_path}\"')"
      ],
      "metadata": {
        "id": "7QdbUpROz4_i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Network"
      ],
      "metadata": {
        "id": "iZWnRjsXzqJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pxcAkJTTbSV",
        "outputId": "cb1c2caa-4dcb-4e61-f09b-d685051110d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19:42:09 START\n",
            "19:42:57 --- Epoch: 0\tTrain loss: 5.8985\tVal loss: 5.4915\tTrain perplexity: 364.4724\tTest perplexity: 242.6160\n",
            "19:43:42 --- Epoch: 1\tTrain loss: 5.5780\tVal loss: 5.4562\tTrain perplexity: 264.5393\tTest perplexity: 234.2148\n",
            "19:44:27 --- Epoch: 2\tTrain loss: 5.4478\tVal loss: 5.4543\tTrain perplexity: 232.2532\tTest perplexity: 233.7622\n",
            "19:45:11 --- Epoch: 3\tTrain loss: 5.4435\tVal loss: 5.5411\tTrain perplexity: 231.2542\tTest perplexity: 254.9671\n",
            "19:45:56 --- Epoch: 4\tTrain loss: 5.5174\tVal loss: 5.6654\tTrain perplexity: 248.9944\tTest perplexity: 288.6917\n"
          ]
        }
      ],
      "source": [
        "model = Net(vocab_size, params['embed_size'], params['hidden_size'], params['num_layers'], params['dropout'], use_lstm=True).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=params['lr_decay'])\n",
        "model = training_loop(corpus.train, corpus.valid, model, optimizer, criterion, lr_scheduler, params, is_gpu_available)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vuBYewv8jqKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ex2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}