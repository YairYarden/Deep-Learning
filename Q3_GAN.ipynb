{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q3_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "GUGdeXHsXv2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime \n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import autograd\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC # For SVM\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "# check if CUDA is available\n",
        "is_gpu_available = torch.cuda.is_available()\n",
        "\n",
        "if not is_gpu_available:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "DIR_PATH = '/content/drive/MyDrive/Deep_learning_05107255/ex3_316168061_313471526'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2r-QSfk6lKi",
        "outputId": "50c46c44-4243-41d8-deeb-7aab79368f95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "ziDCZfBJXoz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/Deep_learning_05107255/ex3_316168061_313471526'\n",
        "\n",
        "# Path for check point\n",
        "ckpts_path = os.path.join(drive_path, 'Q3_GAN', 'checkpoints')\n",
        "os.makedirs(ckpts_path, exist_ok=True)\n",
        "\n",
        "print('Drive Path : ' + drive_path)\n",
        "print('Check points Path : ' + ckpts_path)"
      ],
      "metadata": {
        "id": "BRNUNZfr6tbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define models Consts"
      ],
      "metadata": {
        "id": "LLncnwdWWuk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 64\n",
        "BATCH_SIZE = 64\n",
        "NOISE_SIZE = 128\n",
        "GEN_ITER = 20000\n",
        "CRITIC_ITER = 5\n",
        "OUTPUT_SIZE = 784  # Fashion MNIST size (28, 28)\n",
        "IMAGE_CHANNELS = 1\n",
        "LAMBDA = 10 # Gradient penalty hyperparameter\n"
      ],
      "metadata": {
        "id": "DCUASM5M61HG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GAN Model"
      ],
      "metadata": {
        "id": "fazh9RagXjRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, mode, kernel_size_1=5, kernel_size_2=5):\n",
        "    super(Generator, self).__init__()\n",
        "    self.mode = mode\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(NOISE_SIZE, 4 * 4 * 4 * DIM),\n",
        "        nn.BatchNorm2d(4 * 4 * 4 * DIM) if mode == 'wgan' else nn.Identity(),\n",
        "        nn.ReLU(True)\n",
        "        )\n",
        "    \n",
        "    self.pre_activate1 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(4 * DIM, 2 * DIM, kernel_size_1),\n",
        "        nn.BatchNorm2d(2 * DIM),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.pre_activate2 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(2 * DIM, DIM, kernel_size_2),\n",
        "        nn.BatchNorm2d(2 * DIM),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.pre_activate3 = nn.ConvTranspose2d(DIM, IMAGE_CHANNELS, stride=2)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.fc1(input)\n",
        "    output = output.view(-1, 4 * DIM, 4, 4)\n",
        "    output = self.pre_activate1(output)\n",
        "    print(output.size())\n",
        "    output = output[:, :, :7, :7]\n",
        "    print(output.size())\n",
        "    output = self.pre_activate2(output)\n",
        "    print(output.size())\n",
        "    output = self.pre_activate3(output)\n",
        "    print(output.size())\n",
        "    output = self.sigmoid(output)\n",
        "    return output.view(-1, OUTPUT_SIZE)\n",
        "  \n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, kernel_size=5):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(IMAGE_CHANNELS, DIM, kernel_size, stride=2, padding=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.fc2 = nn.Sequential(\n",
        "        nn.Linear(DIM, 2 * DIM, kernel_size, stride=2, padding=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.fc3 = nn.Sequential(\n",
        "        nn.Linear(2 * DIM, 4 * DIM, kernel_size, stride=2, padding=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.fc4 = nn.Linear(4 * 4 * 4 * DIM, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.fc1(input)\n",
        "    output = self.fc2(output)\n",
        "    output = self.fc3(output)\n",
        "    output = output.view(-1, 4 * 4 * 4 * DIM)\n",
        "    output = self.fc4(output)\n",
        "    return output.view(-1)\n",
        "\n",
        "\n",
        "def calc_gradient_penalty(disc, real_data, fake_data):\n",
        "  alpha = torch.rand(BATCH_SIZE, 1)\n",
        "  alpha = alpha.expand(real_data.size())\n",
        "  alpha = alpha.to(DEVICE) if torch.cuda.is_available() else alpha\n",
        "\n",
        "  interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      interpolates = interpolates.to(DEVICE)\n",
        "  interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "\n",
        "  disc_interpolates = disc(interpolates)\n",
        "\n",
        "  gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                            grad_outputs=torch.ones(disc_interpolates.size()).to(DEVICE) if torch.cuda.is_available() else torch.ones(\n",
        "                                disc_interpolates.size()),\n",
        "                            create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "  gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
        "  return gradient_penalty\n"
      ],
      "metadata": {
        "id": "QOxg6mcKWq5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}